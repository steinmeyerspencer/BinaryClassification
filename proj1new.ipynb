{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81bd0415-b642-4beb-85c0-f9800d452459",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cbf5c4df-fe29-46ce-8a61-724747dccc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib widget\n",
    "from sklearn.datasets import make_blobs\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.regularizers import L2\n",
    "np.set_printoptions(precision=2)\n",
    "import logging\n",
    "logging.getLogger(\"tensorflow\").setLevel(logging.ERROR)\n",
    "tf.autograph.set_verbosity(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7773ec-b2f3-4239-81fa-4497536637d4",
   "metadata": {},
   "source": [
    "# Set a seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6f08f9c-ee56-4e04-a431-a2752e590421",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seed\n",
    "tf.random.set_seed(1234) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66ef183-148d-4019-ab66-4b0fde8924af",
   "metadata": {},
   "source": [
    "# Load in file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "866f8e8a-819c-43df-b371-724c46bd8f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create path\n",
    "file_path = \"shopping.csv\"\n",
    "\n",
    "# read in file\n",
    "df = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc76022-0b9a-4ad2-af8e-a77564711efc",
   "metadata": {},
   "source": [
    "# Create mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f06a1546-67d4-4617-a012-d8cded6da65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create mappings for bools\n",
    "bool_mapping = {\n",
    "    \"TRUE\":1,\n",
    "    \"FALSE\": 0\n",
    "}\n",
    "\n",
    "# create mappings for month\n",
    "month_mapping = {\n",
    "    \"jan\":1,\n",
    "    \"feb\":2,\n",
    "    \"mar\":3,\n",
    "    \"apr\":4,\n",
    "    \"may\":5,\n",
    "    \"june\":6,\n",
    "    \"jul\":7,\n",
    "    \"aug\":8,\n",
    "    \"sep\":9,\n",
    "    \"oct\":10,\n",
    "    \"nov\":11,\n",
    "    \"dec\":12\n",
    "}\n",
    "\n",
    "# create mapping for visitorType\n",
    "visitor_mapping = {\n",
    "    \"returning_visitor\": 1,\n",
    "    \"new_visitor\": 2,\n",
    "    \"other\": 3\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ea3082-6256-4e73-89a2-37d08c5ecbb9",
   "metadata": {},
   "source": [
    "# Change data to usable types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0a5c3fd-cdde-4770-9d42-4c8387c3cbd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change data types to strings\n",
    "df[\"Weekend\"] = df[\"Weekend\"].astype(str)\n",
    "df[\"Revenue\"] = df[\"Revenue\"].astype(str)\n",
    "df[\"VisitorType\"] = df[\"VisitorType\"].astype(str)\n",
    "df[\"Month\"] = df[\"Month\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b37a8d1-3c8d-4c6a-82ef-29a2fa5350a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert strings to lower\n",
    "# convert bools to upper\n",
    "df[\"VisitorType\"] = df[\"VisitorType\"].str.lower()\n",
    "df[\"Month\"] = df[\"Month\"].str.lower()\n",
    "df[\"Weekend\"] = df[\"Weekend\"].str.upper()\n",
    "df[\"Revenue\"] = df[\"Revenue\"].str.upper()\n",
    "\n",
    "# map\n",
    "df[\"VisitorType\"] = df[\"VisitorType\"].map(visitor_mapping)\n",
    "df[\"Month\"] = df[\"Month\"].map(month_mapping)\n",
    "df[\"Weekend\"] = df[\"Weekend\"].map(bool_mapping)\n",
    "df[\"Revenue\"] = df[\"Revenue\"].map(bool_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3350e6c8-947b-4641-97f7-5d28279a9ef6",
   "metadata": {},
   "source": [
    "# Convert dataframe to numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a64b0b02-a791-4493-b4ff-95036fcdfa3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert dataframe to 2d numpy array\n",
    "data_array = df.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684a86e1-09c6-47bd-9952-a7953aa50180",
   "metadata": {},
   "source": [
    "# Deal with outliers\n",
    "In my last file, the outliers were sending the scaled models training and test loss to the moon. I will now deal with the outliers and see if it makes a difference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a88e1a7-f29f-43e7-9e24-d70cdcdceb2e",
   "metadata": {},
   "source": [
    "I will first initialize xtrain and ytrain so that we can examine outliers in the x train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a9a8965-78be-4c0b-a33d-b841e52f9678",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize xTrain, yTrain\n",
    "xtrain = data_array[:,0:17]\n",
    "ytrain = data_array[:,17]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "71d28519-ec5a-42b4-948f-6da120b9ac66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "638\n",
      "Number of rows removed: 485\n",
      "4515\n"
     ]
    }
   ],
   "source": [
    "# debugging\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "\n",
    "# using the z score, i will remove values outside of the threshhold z = 4\n",
    "# 99.7% of values should be between z = (-3,3), so removing these values above 4 should not hurt our model\n",
    "z_scores = np.abs(stats.zscore(xtrain))  \n",
    "outliers = np.where(z_scores > 4)\n",
    "\n",
    "print(len(outliers[1]))\n",
    "\n",
    "unique_rows = np.unique(outliers[0])\n",
    "\n",
    "xtrain = np.delete(xtrain, unique_rows, axis=0)\n",
    "ytrain = np.delete(ytrain, unique_rows, axis=0)\n",
    "\n",
    "print(f\"Number of rows removed: {len(unique_rows)}\")\n",
    "print(len(xtrain))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73f1da1-de7f-49fd-a2a3-0e3095ab6cc5",
   "metadata": {},
   "source": [
    "So, there were 638 outliers(values with above 4 z score) belonging to 485 rows of data in the training set.\n",
    "Removing these values, we still have 4500 observations, but they are much less skewed right. \n",
    "This should help our model train and test better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca8cd53-4c44-4411-b73b-4976da047e5a",
   "metadata": {},
   "source": [
    "# Create minmax, meanNorm, zScore scaling functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b9bab8e9-1c4f-451d-a0b2-699973e41acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature scaling function 1: minMax\n",
    "# (X - xmin) / (xmax - xmin)\n",
    "\n",
    "def minMaxScale(data):\n",
    "    m = data.shape[0]\n",
    "    n = data.shape[1]\n",
    "    scaledData = np.zeros((m,n))\n",
    "    for j in range(n):\n",
    "        xmin = np.min(data[:,j])\n",
    "        xmax = np.max(data[:,j])\n",
    "\n",
    "        # troubleshooting\n",
    "        if (xmin == xmax):\n",
    "            scaledData[:,j] = 0\n",
    "        else:\n",
    "            for i in range(m):\n",
    "                numer = data[i,j] - xmin\n",
    "                denom = xmax - xmin\n",
    "                scaledData[i,j] = numer/denom\n",
    "    return(scaledData)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "30f19ae0-1256-40a0-9973-4d4d71b80248",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature scaling function 2: mean normalization\n",
    "# x - xmean\n",
    "# xmax-xmin\n",
    "\n",
    "def meanNormalization(data):\n",
    "    m = data.shape[0]\n",
    "    n = data.shape[1]\n",
    "    scaledData = np.zeros((m,n))\n",
    "    for j in range(n):\n",
    "        xmin = np.min(data[:,j])\n",
    "        xmax = np.max(data[:,j])\n",
    "        xmean = np.mean(data[:,j])\n",
    "        for i in range(m):\n",
    "            numer = data[i,j] - xmean\n",
    "            denom = xmax - xmin\n",
    "            if denom == 0:\n",
    "                scaledData[i,j] = 0\n",
    "            else:\n",
    "                scaledData[i,j] = numer/denom\n",
    "    return(scaledData)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c28c04a3-c11d-4859-b4c8-6a328323c18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature scaling function 3: z score normalization\n",
    "# x - xmean / std\n",
    "\n",
    "def zScoreNormalization(data):\n",
    "    m = data.shape[0]\n",
    "    n = data.shape[1]\n",
    "    scaledData = np.zeros((m,n))\n",
    "    for j in range(n):\n",
    "        xstd = np.std(data[:,j])\n",
    "        xmean = np.mean(data[:,j])\n",
    "        for i in range(m):\n",
    "            numer = data[i,j] - xmean\n",
    "            denom = xstd\n",
    "            if denom == 0:\n",
    "                scaledData[i,j] = 0\n",
    "            else:\n",
    "                scaledData[i,j] = numer/denom\n",
    "    return(scaledData)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7fbc81-f774-4a75-b217-927acc4c8ae0",
   "metadata": {},
   "source": [
    "# Create models of dif sizes and units per layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f71a8fb9-732f-4192-85ff-3f8f35fd4d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up numbers and array to test diff models\n",
    "array1 = [30,15]\n",
    "\n",
    "array21 = [30,20,10]\n",
    "array22 = [20,12,4]\n",
    "\n",
    "array31 = [40,30,20,10]\n",
    "array32 = [30,22,14,6]\n",
    "array33 = [50,35,23,12]\n",
    "\n",
    "array41 = [40,50,55,40,20]\n",
    "array42 = [50,60,70,40,20]\n",
    "array43 = [60,50,40,30,20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "391b8b19-6b2a-4afb-8c81-2f55c1a21229",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing models\n",
    "\n",
    "def makeModel(numLayers, layerUnitArray):\n",
    "    model = Sequential()\n",
    "\n",
    "    for i in range(numLayers):\n",
    "        model.add(Dense(layerUnitArray[i], activation = \"relu\"))\n",
    "    model.add(Dense(1, activation = \"sigmoid\"))\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1ca59f79-eab0-42e3-a7b7-0f7681895d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = np.empty(9, dtype=object)\n",
    "models[0] = makeModel(2, array1)\n",
    "models[1] = makeModel(3, array21)\n",
    "models[2] = makeModel(3, array22)\n",
    "models[3] = makeModel(4, array31)\n",
    "models[4] = makeModel(4, array32)\n",
    "models[5] = makeModel(4, array33)\n",
    "models[6] = makeModel(5, array41)\n",
    "models[7] = makeModel(5, array42)\n",
    "models[8] = makeModel(5, array43)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19a75d9-369b-43d4-a674-634ebc3a7ce5",
   "metadata": {},
   "source": [
    "# Test models on training data - I will comment this out as it take a long time to run."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77cefe99-783d-4ea9-b685-354754d62b80",
   "metadata": {},
   "source": [
    "for i in range(len(models)):\n",
    "    models[i].compile(\n",
    "        loss = tf.keras.losses.BinaryCrossentropy(),\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "    )\n",
    "    \n",
    "    history = models[i].fit(\n",
    "        xtrain,ytrain,            \n",
    "        epochs=40,\n",
    "    )\n",
    "    finalLoss = history.history['loss'][-1]\n",
    "    print((models[i], finalLoss))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba16179-f805-41db-b7ee-fc1834973ce6",
   "metadata": {},
   "source": [
    "# Make model that produced least loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ed7c65-1003-4003-9f3d-c04d74b51264",
   "metadata": {},
   "source": [
    "After model 4, the loss continually went down. However, a model with 6 layers is likely too complex, so I will use the 6th model. The model\n",
    "with 4 hidden layers and an output layer, with layer sizes of 50,35,23,12 respectively. I will now make the model again on its own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2e199b41-b12e-4ead-8dbe-04bbee74d0aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_9\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_9\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ L1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)                  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">900</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ L2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">35</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,785</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ L3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">23</span>)                  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">828</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ L4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>)                  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">288</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ OL (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                   │              <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ L1 (\u001b[38;5;33mDense\u001b[0m)                           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)                  │             \u001b[38;5;34m900\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ L2 (\u001b[38;5;33mDense\u001b[0m)                           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m35\u001b[0m)                  │           \u001b[38;5;34m1,785\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ L3 (\u001b[38;5;33mDense\u001b[0m)                           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m23\u001b[0m)                  │             \u001b[38;5;34m828\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ L4 (\u001b[38;5;33mDense\u001b[0m)                           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m)                  │             \u001b[38;5;34m288\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ OL (\u001b[38;5;33mDense\u001b[0m)                           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   │              \u001b[38;5;34m13\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,814</span> (14.90 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m3,814\u001b[0m (14.90 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,814</span> (14.90 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m3,814\u001b[0m (14.90 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# make the unregularized model in a prettier way\n",
    "inpSize = xtrain.shape[1]\n",
    "\n",
    "model = Sequential(\n",
    "    [\n",
    "        tf.keras.Input(shape=(inpSize,)),\n",
    "        Dense(50, activation = 'relu',   name = \"L1\"),\n",
    "        Dense(35, activation = 'relu',   name = \"L2\"),\n",
    "        Dense(23, activation = 'relu',   name = \"L3\"),\n",
    "        Dense(12, activation = 'relu',   name = \"L4\"),\n",
    "        Dense(1, activation = 'sigmoid',   name = \"OL\"),\n",
    "    ]\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f9105c-3514-44d5-9023-5444615e5b75",
   "metadata": {},
   "source": [
    "# Compile and fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1898d107-7fdd-47d7-b673-584af42ab847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Loss: 0.2738884687423706\n"
     ]
    }
   ],
   "source": [
    "model.compile(\n",
    "        loss = tf.keras.losses.BinaryCrossentropy(),\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    xtrain,ytrain,\n",
    "    epochs=40,\n",
    "    verbose = 0,\n",
    ")\n",
    "\n",
    "final_loss = history.history['loss'][-1]\n",
    "print(f\"Final Loss: {final_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28678426-1547-4405-96b6-96d480114e3f",
   "metadata": {},
   "source": [
    "Training loss: ~.27"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1308da3b-1e88-4086-b5b8-c2f98d85d7bc",
   "metadata": {},
   "source": [
    "# Now let's compute the test loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d41f8a2d-4786-4c55-8930-6b85a141b322",
   "metadata": {},
   "source": [
    "First I will initialize test set, run it through the same conversions as the training, so the data is usable, and then try the model on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "51f5bcbd-de6a-4ddd-bdb4-194c8d618636",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find test loss of non scaled, unregularized data\n",
    "# first will transform test set to match train set\n",
    "# create path\n",
    "test_path = \"unseen.csv\"\n",
    "\n",
    "# read in file\n",
    "df = pd.read_csv(test_path)\n",
    "\n",
    "# change data types to strings\n",
    "df[\"Weekend\"] = df[\"Weekend\"].astype(str)\n",
    "df[\"Revenue\"] = df[\"Revenue\"].astype(str)\n",
    "df[\"VisitorType\"] = df[\"VisitorType\"].astype(str)\n",
    "df[\"Month\"] = df[\"Month\"].astype(str)\n",
    "\n",
    "# convert strings to lower\n",
    "# convert bools to upper\n",
    "df[\"VisitorType\"] = df[\"VisitorType\"].str.lower()\n",
    "df[\"Month\"] = df[\"Month\"].str.lower()\n",
    "df[\"Weekend\"] = df[\"Weekend\"].str.upper()\n",
    "df[\"Revenue\"] = df[\"Revenue\"].str.upper()\n",
    "\n",
    "# map\n",
    "df[\"VisitorType\"] = df[\"VisitorType\"].map(visitor_mapping)\n",
    "df[\"Month\"] = df[\"Month\"].map(month_mapping)\n",
    "df[\"Weekend\"] = df[\"Weekend\"].map(bool_mapping)\n",
    "df[\"Revenue\"] = df[\"Revenue\"].map(bool_mapping)\n",
    "\n",
    "# convert dataframe to 2d numpy array\n",
    "test_array = df.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a1ffe5-d32c-4af3-af18-080f15aedcfc",
   "metadata": {},
   "source": [
    "Now I will remove outliers of this dataset also, if any are found, after initializing xtest, ytest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f8008a67-4582-4cad-8179-2c74643e3b87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65\n",
      "Number of rows removed: 52\n"
     ]
    }
   ],
   "source": [
    "xtest = test_array[:,0:17]\n",
    "ytest = test_array[:,17]\n",
    "\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "\n",
    "# using the z score, i will remove values outside of the threshhold z = 4\n",
    "# 99.7% of values should be between z = (-3,3), so removing these values above 4 should not hurt our model\n",
    "z_scores = np.abs(stats.zscore(xtest))  \n",
    "outliers = np.where(z_scores > 4)\n",
    "\n",
    "print(len(outliers[1]))\n",
    "\n",
    "unique_rows = np.unique(outliers[0])\n",
    "\n",
    "xtest = np.delete(xtest, unique_rows, axis=0)\n",
    "ytest = np.delete(ytest, unique_rows, axis=0)\n",
    "\n",
    "print(f\"Number of rows removed: {len(unique_rows)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5272197f-6c14-4674-b826-809cea75b00f",
   "metadata": {},
   "source": [
    "So we found 65 outliers in 52 of the rows. I removed these rows. We still have 948 out of our 1000 observations, but should be much less skewed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affb27cf-b870-44e2-90dc-14bfb6c6da88",
   "metadata": {},
   "source": [
    "Now to try our model on unseen data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c42dcc31-fa10-4ae2-aed9-5e69bc3e1d84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3169  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.3279884159564972"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(xtest,ytest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aafe3a5-567f-4cbb-89a7-ec2b5435a3b7",
   "metadata": {},
   "source": [
    "Test loss: ~.36"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe902d2-de6d-4dcd-b6ee-3bac509c66be",
   "metadata": {},
   "source": [
    "# Function to test models for ease and less code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e57e317b-9b26-4a6d-bd77-2f46d1628870",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create function to test models so i can stop copy pasting \n",
    "def fitModel(xt,yt):\n",
    "    model.compile(\n",
    "        loss = tf.keras.losses.BinaryCrossentropy(),\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5, clipnorm=1.0),\n",
    "    )\n",
    "\n",
    "    history = model.fit(\n",
    "        xt,yt,\n",
    "        epochs=100,\n",
    "        verbose = 0,\n",
    "    )\n",
    "    \n",
    "    final_loss = history.history['loss'][-1]\n",
    "    print(final_loss)\n",
    "    return(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3978c3c3-5696-4608-8490-b0983549d762",
   "metadata": {},
   "source": [
    "# Initialize feature scaled training datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cb00387d-5f45-48e6-971a-0be10fb8f99c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.6357728321552973 10.090865202083412\n"
     ]
    }
   ],
   "source": [
    "# initlailize training values for each scaled dataset\n",
    "minMaxXtrain = minMaxScale(xtrain)\n",
    "\n",
    "meanNormXtrain = meanNormalization(xtrain)\n",
    "\n",
    "zScoreXtrain = zScoreNormalization(xtrain)\n",
    "print(np.min(zScoreXtrain), np.max(zScoreXtrain))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0e56cc-f8f5-45fe-99da-7a6eb9bcd7f8",
   "metadata": {},
   "source": [
    "Based on the max z score of 10, there is clearly still an outlier. This is because the mean and std are shifting when the extreme outliers are removed. If I iterated the outlier removal, I would lose over half my data, which I refuse to do."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a249be4-c00b-4ae3-a56f-6bc729d27be7",
   "metadata": {},
   "source": [
    "# Initialize feature scaled test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "037fcc5b-f30b-4db1-b122-fa9cc2686bc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.56910981279221 7.9102912153422364\n"
     ]
    }
   ],
   "source": [
    "# initlailize test values for each scaled dataset\n",
    "minMaxXtest = minMaxScale(xtest)\n",
    "\n",
    "meanNormXtest = meanNormalization(xtest)\n",
    "\n",
    "zScoreXtest = zScoreNormalization(xtest)\n",
    "print(np.min(zScoreXtest), np.max(zScoreXtest))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ac5be2-4b82-44e6-8c9a-f33e2159c433",
   "metadata": {},
   "source": [
    "# Test our model on feature scaled training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e7a43ce9-4087-4b18-9481-5c0839ee529e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.25671860575675964\n"
     ]
    }
   ],
   "source": [
    "minMaxModel = fitModel(minMaxXtrain, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6a6c1c8e-4514-41eb-9fa2-bff51e98fa63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.2962  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.29451480507850647"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minMaxModel.evaluate(minMaxXtest, ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3a9c44de-68be-4857-bbc8-52a41bd2a858",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.23635859787464142\n"
     ]
    }
   ],
   "source": [
    "meanNormModel = fitModel(meanNormXtrain, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2b484047-5325-404b-b513-500132be5a06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2920  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.2870081067085266"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meanNormModel.evaluate(meanNormXtest, ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "61147a1a-33ea-4ac7-b0f2-eb033833f38d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2094227373600006\n"
     ]
    }
   ],
   "source": [
    "zScoreModel = fitModel(zScoreXtrain, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5d1717ff-e7b1-4d22-bee0-f5e15b67f078",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3024  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.29167288541793823"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zScoreModel.evaluate(zScoreXtest, ytest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b78132c-d76b-4310-9663-97e2c124c131",
   "metadata": {},
   "source": [
    "# Results:\n",
    "MinMax scaled training loss: ~0.24   \n",
    "MeanNorm scaled training loss: ~0.22  \n",
    "zScore scaled training loss: ~0.20  \n",
    "  \n",
    "MinMax scaled test loss: ~0.28  \n",
    "MeanNorm scaled test loss: ~0.26  \n",
    "zScore scaled test loss: ~0.29  \n",
    "  \n",
    "I will choose Z Score scaling as my preferred scaling method due to the low losses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611fc89e-39fb-4ea0-a911-52add88644bf",
   "metadata": {},
   "source": [
    "# Regularized Model Initiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "181dc134-6603-4202-aa49-07cfc5347821",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the regularized model in a prettier way\n",
    "inpSize = xtrain.shape[1]\n",
    "\n",
    "regularizedModel = Sequential(\n",
    "    [\n",
    "        tf.keras.Input(shape=(inpSize,)),\n",
    "        Dense(50, activation = 'relu',   name = \"L1\",kernel_regularizer = L2(0.01)),\n",
    "        Dense(35, activation = 'relu',   name = \"L2\",kernel_regularizer = L2(0.01)),\n",
    "        Dense(23, activation = 'relu',   name = \"L3\",kernel_regularizer = L2(0.01)),\n",
    "        Dense(12, activation = 'relu',   name = \"L4\",kernel_regularizer = L2(0.01)),\n",
    "        Dense(1, activation = 'sigmoid',   name = \"OL\",kernel_regularizer = L2(0.01)),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d8e83d-2332-4f16-824f-995d66b812d2",
   "metadata": {},
   "source": [
    "Test regularized model on mean norm scaled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ea2b7024-117a-4731-84f8-b94c13b79675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.386945515871048\n"
     ]
    }
   ],
   "source": [
    "regularizedModel.compile(\n",
    "        loss = tf.keras.losses.BinaryCrossentropy(),\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5, clipnorm=1.0),\n",
    "    )\n",
    "\n",
    "history = regularizedModel.fit(\n",
    "    meanNormXtrain, ytrain,\n",
    "    epochs=100,\n",
    "    verbose = 0,\n",
    ")\n",
    "\n",
    "final_loss = history.history['loss'][-1]\n",
    "print(final_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fadf4039-278f-4b0c-8a5d-37a678abcede",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4002377688884735"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regularizedModel.evaluate(meanNormXtest, ytest, verbose = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72af1004-9529-493a-aa39-057294532407",
   "metadata": {},
   "source": [
    "Not exactly what I wanted, but oh well."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
